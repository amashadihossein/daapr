---
title: "daapr: New project workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{daapr: New project workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Step 1: Initialize the project 

For a new project, start by initializing the project using `dpbuild::dp_init`, which does the following:

1. sets up the folder structure
2. sets up git and switch to specified `branch_name`
3. sets up `renv` to capture package dependencies
4. sets up daap configuration yaml file `daap_config.yaml`

An example would be as follows:

```{r dp_init, eval = F}
library(daapr)

board_params_set_dried  <- fn_dry(board_params_set_s3(board_alias = "cars_board",
                                                      bucket_name = "daap_bucket",
                                                      region = "us-west-1"))


# Dry function call to setting credentials
creds_set_dried <- fn_dry(creds_set_aws(key = Sys.getenv("AWS_KEY"), secret = Sys.getenv("AWS_SECRET") ))

# Initialize dp repo
dp_repo <- dp_init(project_path = "dp_test1",
                   project_description = "Test data product",
                   branch_name = "us001",
                   branch_description = "User story 1",
                   readme_general_note = "This data object is generated for testing purposes",
                   board_params_set_dried = board_params_set_dried,
                   creds_set_dried = creds_set_dried)

```

**NOTE:** `dp_init` builds the yaml config file, `daap_config.yaml`, with all the configurations specified. Configuration includes key:value pairs as well as instruction for function calls. In the above example, two instructions for two function calls are provided. These function call instructions can be thought of as "dried" functions which could be "hydrated" later when executed:

- `board_params_set_s3(board_alias = "cars_board", bucket_name = "daap_bucket",region = "us-west-1")`
- `creds_set_aws(key = Sys.getenv("AWS_KEY"), secret = Sys.getenv("AWS_SECRET") )`

Note, that the 2nd function call relies on "AWS_KEY" and "AWS_SECRET" to be 
available in the environment when the function is beign hydrated

## Step 2: Add starter script

This step is optional, but highly recommended. The starter code includes:

1. `dp_journal.RMD`: A dev journal which will help both guide one through and document the steps in building the data product
2. `dp_make.R`: The main workflow management script the execution of which should build the data product
3. `R/dp_structure.R`: Starter code that will help with structuring the data product

```{r dpcode_add, eval = F}
dpbuild::dpcode_add(project_path = dp_repo)
```


## Step 3: Data prep

**Goal:**: This involves following the steps in the dev journal up until `source("dp_make.R")` step. The goal of this step is to sync the right subset (or all) of the input data into remote and capture the relevant metadata.

For convenience, the directory is switched to project directory just built

```{r switch_dir, eval=FALSE}
setwd(dp_repo)
```


## Step 4: Derive new features as needed

This is where the main logic of building a data product per user story is implemented as functions defined within the `/R` sub-directory of the project, as well as integration of these functions within `dp_make.R` workflow.


## Step 5: Execute `dp_make.R`

Once satisfied with changes needed to derived features, execute the workflow plan (this is included `dp_journal.RMD`)

```{r make_plan, eval= FALSE}
source("dp_make.R")
```

If data testing has been implemented, the test results can be evaluated here and modifications to the code be made as needed.

## Step 6: commit

Once data product made meets the expectation, run `dp_commit` providing the commit message. This will complete one development cycle, making the data product and code ready for deployment.

```{r commit, eval=F}
dpbuild::dp_commit(project_path = ".", commit_description = "First dp build: only input data")
```


## Step 7: Push and deploy

Push the code to GitHub repo. 

- If CI/CD is set up, the build process should pick up the deployment from here. Specifically, remote server will clone the code, build the data product and deploy it to the remote location as specified by the configuration set up during the initialization. 

- In the absence of CI/CD, while GitHub push can be skipped, avoid doing so. First push the code to the repo and then run `dpdeploy::dp_deploy(<project path>)` to deploy the code and metadata representing the data.

